{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3a6753",
   "metadata": {},
   "source": [
    "# 停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964bf0e8",
   "metadata": {},
   "source": [
    "删除没有信息量的单词还有另一种方法，就是舍弃那些出现次数太多以至于没有信息量的单词。有两种主要方法:使用特定语言的停用词 (stopword)列表，或者舍弃那些出现过于频繁的单词。scikit-learn的feature_extraction.text 模块中提供了英语停用词的内置列表:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b4dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['yours', 'afterwards', 'a', 'almost', 'around', 'when', 'via', 'nine', 'mostly', 'off', 'him', 'de', 'take', 'get', 'us', 'couldnt', 'full', 'thereupon', 'who', 'me', 'that', 'eight', 'become', 'even', 'several', 'being', 'no', 'else', 'anyhow', 'across', 'my', 'serious']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae16bb",
   "metadata": {},
   "source": [
    "显然，删除上述列表中的停用词只能使特征数量减少318 个 (即上述列表的长度)，但可能会提高性能。我们来试一下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38525220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "reviews_train = load_files(\"E:/clone/machine-learning/data/aclImdb/train/\")\n",
    "\n",
    "# load_files返回一个Bunch对象，其中包含训练文本和训练标签\n",
    "data_array = np.array(reviews_train.data)\n",
    "target_array = np.array(reviews_train.target)\n",
    "\n",
    "# Filter out documents where the target is not equal to 2\n",
    "labeled_indices = np.where(target_array != 2)[0]\n",
    "text_train, y_train = data_array[labeled_indices], target_array[labeled_indices]\n",
    "text_train = [doc.replace(b'<br />',b' ') for doc in text_train]\n",
    "\n",
    "reviews_test = load_files(\"E:/clone/machine-learning/data/aclImdb/test/\")\n",
    "data_array = np.array(reviews_test.data)\n",
    "target_array = np.array(reviews_test.target)\n",
    "text_test, y_test = data_array, target_array\n",
    "text_test = [doc.replace(b'<br />',b' ') for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0611579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# 指定stop_words=\"english\"将使用内置列表\n",
    "# 我们也可以扩展这个列表并传人我们自己的列表\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66962277",
   "metadata": {},
   "source": [
    "现在数据集中的特征数量减少了 305 个 (27271-26966)，说明大部分停用词(但不是所有)都出现了。我们再次运行网格搜索:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd81c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.884\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=800), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78843d27",
   "metadata": {},
   "source": [
    "使用停用词后的网格搜索性能略有下降一不至于担心，但鉴于从 27 000 多个特征中删除 305 个不太可能对性能或可解释性造成很大影响，所以使用这个列表似乎是不值得的.固定的列表主要对小型数据集很有帮助，这些数据集可能没有包含足够的信息，模型从数据本身无法判断出哪些单词是停用词"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
